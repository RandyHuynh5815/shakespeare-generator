{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Thy Own Markov Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all the math thrown at you, you may think that building a Markov chain would be extremely painful. In actuality, it is simpler than you think, and there are MULTIPLE ways to do it. We are going to implement an autocomplete program for Shakespeare's works. Let us walk through the progress of doing so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Act I: Setting Up The Scene"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should be a given, but we got to import the packages we may need in order to get this to work. Before running the imports, make sure to set up the environment using the requirements files. To install the neccessary packages, just simply write the command in the terminal.  \n",
    "\n",
    "pip install -r requirements.txt\n",
    "\n",
    "Afterwards, run the following block of code to ensure everything is installed properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm that you are in the right directory by running the code block below. It should end with 'shakespeare-generator'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\randy\\Productivity\\shakespeare-generator-answers\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's double-check to see if all of our data is there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1henryiv.txt',\n",
       " '1henryvi.txt',\n",
       " '2henryiv.txt',\n",
       " '2henryvi.txt',\n",
       " '3henryvi.txt',\n",
       " 'allswell.txt',\n",
       " 'asyoulikeit.txt',\n",
       " 'cleopatra.txt',\n",
       " 'comedy_errors.txt',\n",
       " 'coriolanus.txt',\n",
       " 'cymbeline.txt',\n",
       " 'hamlet.txt',\n",
       " 'henryv.txt',\n",
       " 'henryviii.txt',\n",
       " 'john.txt',\n",
       " 'julius_caesar.txt',\n",
       " 'lear.txt',\n",
       " 'lll.txt',\n",
       " 'macbeth.txt',\n",
       " 'measure.txt',\n",
       " 'merchant.txt',\n",
       " 'merry_wives.txt',\n",
       " 'midsummer.txt',\n",
       " 'much_ado.txt',\n",
       " 'othello.txt',\n",
       " 'pericles.txt',\n",
       " 'richardii.txt',\n",
       " 'richardiii.txt',\n",
       " 'romeo_juliet.txt',\n",
       " 'taming_shrew.txt',\n",
       " 'tempest.txt',\n",
       " 'timon.txt',\n",
       " 'titus.txt',\n",
       " 'troilus_cressida.txt',\n",
       " 'twelfth_night.txt',\n",
       " 'two_gentlemen.txt',\n",
       " 'winters_tale.txt']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "oh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's quickly learn how to collect the data we would need in the first place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aside: Scrape Thy Data From the Hands of Shakespeare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to do this, we will go back to the presentation and then head over to scraper.py to test your knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we shall check our data's prescence once again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1henryiv.txt',\n",
       " '1henryvi.txt',\n",
       " '2henryiv.txt',\n",
       " '2henryvi.txt',\n",
       " '3henryvi.txt',\n",
       " 'allswell.txt',\n",
       " 'asyoulikeit.txt',\n",
       " 'cleopatra.txt',\n",
       " 'comedy_errors.txt',\n",
       " 'coriolanus.txt',\n",
       " 'cymbeline.txt',\n",
       " 'hamlet.txt',\n",
       " 'henryv.txt',\n",
       " 'henryviii.txt',\n",
       " 'john.txt',\n",
       " 'julius_caesar.txt',\n",
       " 'lear.txt',\n",
       " 'lll.txt',\n",
       " 'macbeth.txt',\n",
       " 'measure.txt',\n",
       " 'merchant.txt',\n",
       " 'merry_wives.txt',\n",
       " 'midsummer.txt',\n",
       " 'much_ado.txt',\n",
       " 'othello.txt',\n",
       " 'pericles.txt',\n",
       " 'richardii.txt',\n",
       " 'richardiii.txt',\n",
       " 'romeo_juliet.txt',\n",
       " 'taming_shrew.txt',\n",
       " 'tempest.txt',\n",
       " 'timon.txt',\n",
       " 'titus.txt',\n",
       " 'troilus_cressida.txt',\n",
       " 'twelfth_night.txt',\n",
       " 'two_gentlemen.txt',\n",
       " 'winters_tale.txt']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hurrah! Our data directory is plentiful. Now to begin the Markov chain creation process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Act II: Sentencing Thy Text Into Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now given large amounts of texts, let's make it digestable if we split all of it into sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_TODO_**: _We need a way to extract the sentences from all the text given to us. Let us be bold and do so FROM SCRATCH! But first we need to set some requirements/steps in order for us to be on the same page._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Iterate through every text file in our newly created data folder.\n",
    "2. Open each text file and read all the text at once.\n",
    "3. To normalize the sentences, we replace every whitespace with a space and every ending punctuation with a period. (We will still maintain the commas and semicolons and such for now.)\n",
    "4. Split by period."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Replacing standards**\n",
    "1. ['\\r', '\\n', '\\t', '-'] -> ' '\n",
    "2. ['?', '!'] -> '.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hints**\n",
    "1. os.listdir(dir: str) -> List[str] = Retrieves a list of files found within the directory. (Used above)\n",
    "2. s.split(char: str) -> List[str] = Splits s by a char and gives you a list of all segments from the split\n",
    "3. s.replace(old, new) -> str = Replaces every instance of old with new in s\n",
    "4. l1.extends(l2) -> Appends all the elements from l2 to l1\n",
    "5. with open(file, 'r') as f: = Open the file as f in for the purposes of reading it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = list()\n",
    "n = 10 # Feel free to play around with this value.\n",
    "\n",
    "for text in os.listdir('data')[:5]:\n",
    "    with open('data/' + text, 'r') as f:\n",
    "        data = f.read()\n",
    "        clean = data.replace('\\r', ' ').replace('\\n', ' ').replace('-', ' ').replace('\\t', ' ').replace('?', '.').replace('!', '.').split(\".\")\n",
    "        sentences.extend(clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note: For the purposes of this workshop, only take a small subset of the files or even just one file in the data folder. Setting up a Markov chain for all the data would take an extremely long time. If you have time, you can run the entire folder in your own time._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After writing your function, let's see the sentences we have created by running the code block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 sentences\n",
      "------------------------\n",
      "1) Enter KING HENRY, LORD JOHN OF LANCASTER, the EARL of WESTMORELAND, SIR WALTER BLUNT, and others  So shaken as we are, so wan with care, Find we a time for frighted peace to pant, And breathe short winded accents of new broils To be commenced in strands afar remote.\n",
      "2) No more the thirsty entrance of this soil Shall daub her lips with her own children's blood; Nor more shall trenching war channel her fields, Nor bruise her flowerets with the armed hoofs Of hostile paces: those opposed eyes, Which, like the meteors of a troubled heaven, All of one nature, of one substance bred, Did lately meet in the intestine shock And furious close of civil butchery Shall now, in mutual well beseeming ranks, March all one way and be no more opposed Against acquaintance, kindred and allies: The edge of war, like an ill sheathed knife, No more shall cut his master.\n",
      "3) Therefore, friends, As far as to the sepulchre of Christ, Whose soldier now, under whose blessed cross We are impressed and engaged to fight, Forthwith a power of English shall we levy; Whose arms were moulded in their mothers' womb To chase these pagans in those holy fields Over whose acres walk'd those blessed feet Which fourteen hundred years ago were nail'd For our advantage on the bitter cross.\n",
      "4) But this our purpose now is twelve month old, And bootless 'tis to tell you we will go: Therefore we meet not now.\n",
      "5) Then let me hear Of you, my gentle cousin Westmoreland, What yesternight our council did decree In forwarding this dear expedience.\n",
      "6) My liege, this haste was hot in question, And many limits of the charge set down But yesternight: when all athwart there came A post from Wales loaden with heavy news; Whose worst was, that the noble Mortimer, Leading the men of Herefordshire to fight Against the irregular and wild Glendower, Was by the rude hands of that Welshman taken, A thousand of his people butchered; Upon whose dead corpse there was such misuse, Such beastly shameless transformation, By those Welshwomen done as may not be Without much shame retold or spoken of.\n",
      "7) It seems then that the tidings of this broil Brake off our business for the Holy Land.\n",
      "8) This match'd with other did, my gracious lord; For more uneven and unwelcome news Came from the north and thus it did import: On Holy rood day, the gallant Hotspur there, Young Harry Percy and brave Archibald, That ever valiant and approved Scot, At Holmedon met, Where they did spend a sad and bloody hour, As by discharge of their artillery, And shape of likelihood, the news was told; For he that brought them, in the very heat And pride of their contention did take horse, Uncertain of the issue any way.\n",
      "9) Here is a dear, a true industrious friend, Sir Walter Blunt, new lighted from his horse.\n",
      "10) Stain'd with the variation of each soil Betwixt that Holmedon and this seat of ours; And he hath brought us smooth and welcome news.\n"
     ]
    }
   ],
   "source": [
    "sentences = [s.strip() for s in sentences]\n",
    "print(f\"First {n} sentences\")\n",
    "print(\"------------------------\")\n",
    "for num, sent in enumerate(sentences[:n], 1):\n",
    "    print(f\"{num}) {sent}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Huzzah! We have our sentences. Now we shall build our markov chains using not one, but TWO different ways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Act III: Lord Ihler's and His Matrix Servant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Professor Alexander Ihler is one of our advisors here for AI@UCI. He is currently teaching CS 179, which is a class about graphical models, where different variables express their dependencies to each other in the form of a graph. One graphical model that has been discussed are Markov chains (woah, we're doing a workshop on this). Like what we talked about earlier, we can represent our Markov chain with a transition matrix, which is a matrix of probabilities to get from one state to another. Let's see how we can use our knowledge to create one here. Once again, from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all let's create a list of tokens. A token is a small unit of a piece of text and is essential in Natural Language Processing, a branch of AI. For our purposes, a token will be a word in the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_TODO_**: _Let us create a list of tokens based on the sentences list that we have created earlier._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Iterate through each sentence\n",
    "2. Remove any excess punctuation [',', ';', ':'] and replace them with a space\n",
    "3. Split the sentence into tokens (tokenization) and ensure that each token is lowercase\n",
    "4. Filter out the non alphanumeric tokens\n",
    "5. Add those tokens into our token list (maintaining their order - this is very important)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hints**\n",
    "1. s.lower() -> str = Returns a lowercase version of the string s\n",
    "2. s.isalpha() -> bool = Returns a boolean that indicates if a value is alphanumeric\n",
    "3. filter(function, l) -> l2 = Returns a filtered list l2 where it only contains elements e in l where function(e) is True\n",
    "3. Previous hints given before can also help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = list()\n",
    "for s in sentences:\n",
    "    clean_s = s.lower().replace(',', ' ').replace(';', ' ').replace(':', ' ').split()\n",
    "    clean_s = filter(lambda x: x.isalpha(), clean_s)\n",
    "    tokens.extend(clean_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**Theory Review**_: _What will be the size of our transition matrix?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hints**  \n",
    "1. np.unique(a) -> unique_elem, counts = Given an np.array, it returns an array of unique elements and their counts. Both are np.arrays\n",
    "2. You might also want to make a list of unique elements as well, we will need them soon :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8943, 8943)\n"
     ]
    }
   ],
   "source": [
    "# Write your answer here (show your work), your answer should be stored in a 2-tuple (row, col)\n",
    "unq_tokens = list(np.unique(tokens))\n",
    "unq = len(np.unique(tokens))\n",
    "size = (unq,unq)\n",
    "print(size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note: Depending on which subset of the data you have gotten, you may have different answers from everyone else. As long as you follow the correct process and have correct reasoning, you would be correct._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have a list of tokens, you can now create a transition matrix. So let's do that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_TODO_**: _Let us create a transition matrix to represent our Markov chain._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Initialize our transition matrix with a numpy matrix of zeros of size size (your theory answer comes in handy doesn't it)\n",
    "2. Initialize the first term (arrow) with the first word\n",
    "3. Populate the matrix while iterating through our token list\n",
    "4. Normalize the probabilities in each column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hints**\n",
    "1. np.zeros(size) -> np.array = Creates a matrix of size size (2-value tuple) filled with 0s\n",
    "2. l.index(e) -> x = Returns the index x of element e in the list l\n",
    "3. M[i,:] -> Row[i] = Gathers all the elements from row i\n",
    "4. Operations on rows or columns will distribute to all elements in it (Ex. Row[i] += 1 will add one to every element in row i)\n",
    "5. sum(iterable) -> int/float = Sums up every element in an iterable (list, tuple)\n",
    "6. Remember the list of unique tokens you made from the theory question? :D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Write your TODO here\n",
    "T = np.zeros(size)\n",
    "start = tokens[0]\n",
    "for t in tokens[1:]:\n",
    "    starting_state = unq_tokens.index(start)\n",
    "    ending_state = unq_tokens.index(t)\n",
    "    T[starting_state][ending_state] += 1\n",
    "    start = t\n",
    "for i in range(size[0]):\n",
    "    T[i,:] /= sum(T[i,:])\n",
    "print(T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our matrix, let's actually create an autocomplete and generate some text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_TODO_**: _Build an autocomplete or text generator using our transition matrix_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Initialize a start state (starting word) - Can be anything but if you dont have an idea, use \"thou\"\n",
    "2. Have a set length n (default 10 but can be anything you want), and in each iteration choose the next word based on your current word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hints**\n",
    "1. np.random.choice(l, p=probs) -> e = Returns a random element e from a list l with weighted probabilities probs for each element.  \n",
    "_Note: len(l) == len(probs) and sum(probs) == 1_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 'thou'\n",
    "n = 10\n",
    "\n",
    "chain = [start]\n",
    "for i in range(n):\n",
    "    start = np.random.choice(unq_tokens, p=T[i,:])\n",
    "    chain.append(start)\n",
    "generated_text = ' '.join(chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now see the gifts of your work by running the code block below and have fun by rerunning the code block above. It's not going to generate the same string everytime (unless you set a seed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thou friend our all if war their with what people to\n"
     ]
    }
   ],
   "source": [
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Act IV: King Yurichev, His Library of Dictionaries and N of His Grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dennis Yurichev is a computer scientist who wrote \"Reverse Engineering for Beginners\". He wrote a blog post about creating a Markov chain autocomplete based on a given text. The theory of Markov chains is still present in this implementation. However, his approach is different from Ihler's as he uses dictionaries of dictionaries of counts rather than forming a transition matrix in order to generate words. We will look at his implementation as a review of the concepts used from the last act."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code block below to setup and get started on the next act."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stat(t, n=5):\n",
    "    total=float(sum(t.values()))\n",
    "    s=sorted(t.items(), key=lambda item: item[1], reverse=True)\n",
    "    for pair in s[:n]:\n",
    "        print (\"%s %d%%\" % (pair[0], (float(pair[1])/total)*100))\n",
    "\n",
    "first=defaultdict(lambda: defaultdict(int)) # single word keys (Ex. \"to\")\n",
    "second=defaultdict(lambda: defaultdict(int)) # two word phrases as keys (Ex. \"to be\")\n",
    "third=defaultdict(lambda: defaultdict(int)) # three word phrases as keys (Ex. \"not to be\")\n",
    "\n",
    "# Sample example of what first could be\n",
    "# {\"to\": {\"be\": 1, \"meet\": 1}, \"or\": {\"not\": 1}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_TODO_**: _Let's update his dictionaries by filling our his update_occ function, and then populating the dictionary with connections and weights needed._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hints**\n",
    "1. Each dictionary (first, second, third) is in the form {phrase: {next_word: count}}\n",
    "2. Use the last 1, 2 or 3 words to create the phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_occ(d, seq, w):\n",
    "    \"\"\"\n",
    "    Params:\n",
    "        d: dict - Dictionary to be updated\n",
    "        seq: str - The sequence that represents the previous state\n",
    "        w: str - The words that represent the current state\n",
    "    Returns:\n",
    "        None\n",
    "    \n",
    "    This function updates the dictionary's count during token iteration\n",
    "    \"\"\"\n",
    "    # Fill out function implementation here\n",
    "    d[seq][w] += 1\n",
    "\n",
    "for s in sentences:\n",
    "    words = s.replace(',', ' ').replace(';', ' ').replace(\":\", ' ').split() # TODO: Split each sentence into words\n",
    "    words = list(filter(lambda x: x.isalpha(), words))\n",
    "    if len(words)==0:\n",
    "        continue\n",
    "    for i in range(len(words)):\n",
    "        # only two words available:\n",
    "        if i>=1:\n",
    "            update_occ(first, words[i-1], words[i]) #TODO: update the occurences -> {\"to\": {\"be\": 1}}\n",
    "        # three words available:\n",
    "        if i>=2:\n",
    "            update_occ(second, words[i-2] + \" \" + words[i-1], words[i]) #TODO: update the occurences -> {\"to be\": {\"or\": 1}}\n",
    "        # four words available:\n",
    "        if i>=3:\n",
    "            update_occ(third, words[i-3] + \" \" + words[i-2] + \" \" + words[i-1], words[i]) #TODO: update the occurences -> {\"not to be\": {\"that\": 1}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your code by running the block below. You can change test to any phrase you would want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* second order. for sequence: sight did\n",
      "ravish 100%\n",
      "* first order. for word: did\n",
      "not 6%\n",
      "he 5%\n",
      "I 5%\n",
      "you 4%\n",
      "but 2%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test = \"her sight did\"\n",
    "test_words=test.split(\" \")\n",
    "\n",
    "test_len=len(test_words)\n",
    "last_idx=test_len-1\n",
    "\n",
    "if test_len>=3:\n",
    "    tmp=test_words[last_idx-2]+\" \"+test_words[last_idx-1]+\" \"+test_words[last_idx]\n",
    "    if tmp in third:\n",
    "        print (\"* third order. for sequence:\",tmp)\n",
    "        print_stat(third[tmp])\n",
    "\n",
    "if test_len>=2:\n",
    "    tmp=test_words[last_idx-1]+\" \"+test_words[last_idx]\n",
    "    if tmp in second:\n",
    "        print (\"* second order. for sequence:\", tmp)\n",
    "        print_stat(second[tmp])\n",
    "\n",
    "if test_len>=1:\n",
    "    tmp=test_words[last_idx]\n",
    "    if tmp in first:\n",
    "        print (\"* first order. for word:\", tmp)\n",
    "        print_stat(first[tmp])\n",
    "print (\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us complete his autocomplete. It will be a similar process to what we have done before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_TODO_**: _Let's finish his selection function and use it to put recreate his autocomplete algorithm._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Finish the gen_random_from_tbl function\n",
    "2. Fill out the missing pieces of the for loop for text generation using gen_random_from_tbl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hints**\n",
    "1. random.choices(l, weights=w) -> e = Returns a random element e from a list l with weights/bias w  \n",
    "_Note: len(l) == len(w) but sum(w) not necessariliy equal to 1_\n",
    "2. Use the last 1, 2 or 3 words to create the phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upon whose majesty That Cardinal Beaufort is at point of death For suddenly a grievous sickness took him That makes him close his eyes and methought he had made two holes in the ale new petticoat and so peeped through force of your report My noble Lord of Suffolk or for that My tender youth was never yet attaint With any passion of inflaming love I cannot tell but this I am assured I feel such sharp dissension in my breast And what I do A witch by fear not force like Hannibal Drives back our troops and conquers as she lists So bees with smoke and doves with noisome stench Are from their hives and houses driven away what instinct hadst thou for it straight a thing impossible To compass wonders but by help of devils and hell have through the very middest of you grant that my poor virtue grant that words bewitch him not come here no by my faith I think you are fallen into the disease for you hear not what I did I did in honour Led by the impartial conduct of my soul And till I root out their accursed line And leave not one alive\n"
     ]
    }
   ],
   "source": [
    "text = ['upon', 'whose', 'majesty']\n",
    "\n",
    "def gen_random_from_tbl(t):\n",
    "    \"\"\"\n",
    "    Params:\n",
    "        d: dict - Dictionary whose value we are taking from. It should be in the form {word (str): weight (int)}\n",
    "    Returns:\n",
    "        str - Random word generated from d\n",
    "        \n",
    "    Given a dictionary with word-weight pairs, generate a random word from the dictionary and return it.\n",
    "    \"\"\"\n",
    "    # TODO: Complete the function (Can you do it in one line?)\n",
    "    return random.choices(list(t.keys()), weights=list(t.values()))[0]\n",
    "\n",
    "text_len=len(text)\n",
    "\n",
    "# generate at most 100 words:\n",
    "for i in range(200):\n",
    "    last_idx=text_len-1\n",
    "    tmp3= text[last_idx-2] + \" \" + text[last_idx-1] + \" \" + text[last_idx] #TODO: Create 3 word phrase using last_idx\n",
    "    tmp2= text[last_idx-1] + \" \" + text[last_idx] #TODO: Create 2 word phrase using last_idx\n",
    "    tmp1= text[last_idx] #TODO: Create 1 word phrase using last_idx\n",
    "    if tmp3 in third:\n",
    "        new_word=gen_random_from_tbl(third[tmp3]) #TODO: Generate a new word\n",
    "    elif tmp2 in second:\n",
    "        new_word=gen_random_from_tbl(second[tmp2]) #TODO: Generate a new word\n",
    "    elif tmp1 in first:\n",
    "        new_word=gen_random_from_tbl(first[tmp1]) #TODO: Generate a new word\n",
    "    else:\n",
    "        break # dead end\n",
    "    text.append(new_word)\n",
    "    text_len=text_len+1\n",
    "\n",
    "print (\" \".join(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Act V: Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you may have seen, although Ihler's and Yurichev's used different implementation, the basic idea of Markov chains are the same. Now in a discussion format all together, let's answer the following questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What are the similarities between Ihler's and Yurichev's implementations?\n",
    "2. What are the differences between Ihler's and Yurichev's implementations?\n",
    "3. Which implementation would you prefer and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thank you for attending this workshop, and I hope you left this knowing something new."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exeunt"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "36a238e2fb38a3355e131d7eb6061c6fa5c575363fd8baa67f27a42dbc891b6d"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
